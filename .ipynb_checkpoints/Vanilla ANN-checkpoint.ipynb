{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    \n",
    "    def __init__(self, layers, act_funcs):\n",
    "        \n",
    "        self.epsilon = 1e-6\n",
    "        self.layers = layers\n",
    "        self.act_funcs = act_funcs\n",
    "        self.act_func_map = {'relu':self.ReLU, 'sigmoid':self.sigmoid, 'tanh':self.tanh, 'none':self.none, 'softmax':self.softmax}\n",
    "        self.act_diff_map = {'relu':self.ReLU_diff, 'sigmoid':self.sigmoid_diff, 'tanh':self.tanh_diff, 'softmax':self.softmax_diff}\n",
    "        self.loss_error_map = {'mse':self.mse_error, 'binary_crossentropy':self.bincross_error, 'categorical_crossentropy':self.catcross_error}\n",
    "        self.metric_map = {'mse':self.calc_mse, 'binary_crossentropy':self.calc_bincross, 'categorical_crossentropy':self.calc_catcross, 'accuracy':self.calc_accuracy}\n",
    "        \n",
    "        self.weights = [ [] for _ in range(len(self.layers)) ]\n",
    "        self.biases = [ [] for _ in range(len(self.layers)) ]  \n",
    "        self.activations = [ [] for _ in range(len(self.layers)) ]\n",
    "        \n",
    "        for i in range(1, len(layers)):\n",
    "            \n",
    "            self.weights[i] = np.random.randn(layers[i-1], layers[i]) * np.sqrt(2/layers[i])\n",
    "            self.biases[i] = np.zeros(layers[i])\n",
    "            \n",
    "    def tanh(self, X):\n",
    "        return np.divide( (np.exp(X) - np.exp(-X)), (np.exp(X) + np.exp(-X) + self.epsilon) )\n",
    "    \n",
    "    def none(self, X):\n",
    "        return X\n",
    "    \n",
    "    def sigmoid(self, X):\n",
    "        f = np.vectorize(self.stable_sigmoid)\n",
    "        return f(X)\n",
    "        \n",
    "    def stable_sigmoid(self, x):\n",
    "        if x >= 0:\n",
    "            z = np.exp(-x)\n",
    "            return 1 / (1 + z)\n",
    "        else:\n",
    "            z = np.exp(x)\n",
    "            return z / (1 + z)\n",
    "    \n",
    "    def ReLU(self, X):\n",
    "        return np.maximum(X, 0)\n",
    "    \n",
    "    def softmax(self, X):\n",
    "        b = np.mean(X)\n",
    "        return np.exp(X - b)/np.sum(np.exp(X - b))\n",
    "    \n",
    "    def tanh_diff(self, X):\n",
    "        return 1 - np.square(self.tanh(X))\n",
    "    \n",
    "    def sigmoid_diff(self, X):\n",
    "        return np.multiply(self.sigmoid(X), (1-self.sigmoid(X)))\n",
    "    \n",
    "    def ReLU_diff(self, X):\n",
    "        X[X<=0] = 0\n",
    "        X[X>0] = 1\n",
    "        return X\n",
    "    \n",
    "    def softmax_diff(self, X):\n",
    "        return np.multiply(self.softmax(X), (1-self.softmax(X)))\n",
    "        \n",
    "    def fit(self, X, y, **kwargs):  \n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = np.expand_dims(y, 1)\n",
    "          \n",
    "        self.epochs = kwargs.get('epochs', None)    \n",
    "        self.eta = kwargs.get('learning_rate', None)    \n",
    "        self.valid = kwargs.get('validation_data', None)  \n",
    "        self.batch_size = kwargs.get('batch_size', self.epochs)  \n",
    "        \n",
    "        self.history = {}\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            self.history[metric] = [] \n",
    "            if self.valid != None:\n",
    "                self.history[f'val_{metric}'] = []   \n",
    "                if len(self.valid[1].shape) == 1:\n",
    "                    self.valid[1] = np.expand_dims(self.valid[1], 1)\n",
    "            \n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            \n",
    "            n_batches = np.ceil(self.epochs/self.batch_size).astype(int)       \n",
    "            i = 0     \n",
    "            \n",
    "            for _ in range(n_batches):  \n",
    "                \n",
    "                if i+self.batch_size > len(X):        \n",
    "                    batch_inp = X[i:]  \n",
    "                    batch_targets = y[i:]\n",
    "                else:  \n",
    "                    batch_inp = X[i:i+self.batch_size]\n",
    "                    batch_targets = y[i:i+self.batch_size]\n",
    "                    \n",
    "                self.forward_pass(batch_inp)\n",
    "                self.back_propagation(batch_targets)\n",
    "                i += self.batch_size\n",
    "            \n",
    "            for metric in self.metrics:\n",
    "                self.history[metric].append(self.metric_map[metric](batch_inp, batch_targets))\n",
    "                if self.valid != None:\n",
    "                    self.history[f'val_{metric}'].append(self.metric_map[metric](self.valid[0], self.valid[1]))\n",
    "            \n",
    "            print(f'\\nEpoch {epoch} ==> ', end='')\n",
    "            for k,v in self.history.items():\n",
    "                print(f'{k}: {v[-1]}', end=' ')\n",
    "            \n",
    "    def forward_pass(self, X):  \n",
    "        self.activations[0] = X\n",
    "        for i in range(1, len(self.layers)):\n",
    "            act_func = self.act_func_map[self.act_funcs[i]]\n",
    "            self.new = np.dot(self.activations[i-1], self.weights[i]) + self.biases[i]\n",
    "            self.activations[i] = act_func(np.dot(self.activations[i-1], self.weights[i]) + self.biases[i])\n",
    "            \n",
    "    def back_propagation(self, y):\n",
    "        \n",
    "        y_hat = self.activations[-1]\n",
    "        e = self.loss_func_error(y, y_hat)\n",
    "        \n",
    "        for i in range(len(self.weights)-1, 0, -1):\n",
    "            \n",
    "            if i<len(self.weights)-1:\n",
    "                e = np.dot(e, self.weights[i+1].T)\n",
    "                \n",
    "            if self.act_funcs[i] != 'none' :\n",
    "                act_diff = self.act_diff_map[self.act_funcs[i]](self.activations[i])\n",
    "                e = np.multiply(e, act_diff)\n",
    "            \n",
    "            bias_gradients = self.clip_gradients(np.sum(e, axis=0))\n",
    "            weight_gradients = self.clip_gradients(np.dot(self.activations[i-1].T, e))\n",
    "            self.weights[i] = self.weights[i] - self.eta*weight_gradients\n",
    "            self.biases[i] = self.biases[i] - self.eta*bias_gradients\n",
    "        \n",
    "    def compile_network(self, loss, *args):\n",
    "        \n",
    "        if len(args) == 1:\n",
    "            self.metrics = args[0]\n",
    "        else:\n",
    "            self.metrics = []\n",
    "        self.metrics.insert(0, loss)\n",
    "        self.loss_func_error = self.loss_error_map[loss]\n",
    "        self.loss = loss\n",
    "        \n",
    "    def predict(self, X):\n",
    "        self.forward_pass(X)\n",
    "        if self.loss == 'binary_crossentropy':\n",
    "            return (self.activations[-1] > 0.5).astype(int)\n",
    "        else:\n",
    "            return self.activations[-1]\n",
    "        \n",
    "    def clip_gradients(self, grad):\n",
    "        grad[grad>1] = 1\n",
    "        grad[grad<-1] = -1\n",
    "        return grad\n",
    "        \n",
    "    def mse_error(self, y, y_hat):      \n",
    "        n_samples = y.shape[0]\n",
    "        e = -(2/n_samples)*(y-y_hat)\n",
    "        return e\n",
    "            \n",
    "    def bincross_error(self, y, y_hat):     \n",
    "        n_samples = y.shape[0]\n",
    "        e = (1/n_samples)*( -np.divide(y, y_hat+ self.epsilon) + np.divide((1-y), (1-y_hat+ self.epsilon)) )   \n",
    "        return e\n",
    "    \n",
    "    def catcross_error(self, y, y_hat):     \n",
    "        n_samples = y.shape[0]\n",
    "        n_classes = y.shape[1]\n",
    "        e = (1/n_samples*n_classes)*( -np.divide(y, y_hat+ self.epsilon) )   \n",
    "        return e\n",
    "    \n",
    "    def calc_mse(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "        y_hat = self.predict(X)\n",
    "        return np.sum(np.square(y - y_hat))/n_samples\n",
    "    \n",
    "    def calc_bincross(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "        self.forward_pass(X)\n",
    "        y_hat = self.activations[-1]\n",
    "        return np.sum(np.multiply(y, np.log(y_hat + self.epsilon)) + np.multiply((1-y), np.log(1-y_hat + self.epsilon)) ) / n_samples\n",
    "    \n",
    "    def calc_catcross(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "        n_classes = y.shape[1]\n",
    "        self.forward_pass(X)\n",
    "        y_hat = self.activations[-1]\n",
    "        return np.sum(np.multiply(y, np.log(y_hat + self.epsilon)))/(n_samples*n_classes)\n",
    "    \n",
    "    def calc_rmse(self, X, y):\n",
    "        return np.sqrt(self.calc_mse(X, y))    \n",
    "        \n",
    "    def calc_r2(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "        y_hat = self.predict(X)\n",
    "        r2 = 1 - (np.sum(np.square(y - y_hat))) / (np.sum(np.square(y - np.mean(y))))\n",
    "        return r2\n",
    "    \n",
    "    def calc_accuracy(self, X, y):\n",
    "        n_samples = y.shape[0]\n",
    "        if self.loss == 'binary_crossentropy':\n",
    "            y_hat = (self.predict(X) > 0.5).astype(int)\n",
    "            return np.sum(y == y_hat) / n_samples\n",
    "        elif self.loss == 'categorical_crossentropy':\n",
    "            y_hat = np.argmax(self.predict(X))\n",
    "            return np.sum(np.argmax(y) == y_hat) / n_samples\n",
    "        \n",
    "    def evaluate(self, X, y):\n",
    "        \n",
    "        if len(y.shape) == 1:\n",
    "            y = np.expand_dims(y, 1)\n",
    "        \n",
    "        if self.loss == 'mse':\n",
    "            print(f\"Root mean square error: {self.calc_rmse(X, y)}\")\n",
    "            print(f\"R2 score: {self.calc_r2(X, y)}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Accuracy: {self.calc_accuracy(X, y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "\n",
    "#X = np.random.rand(15000, 5)\n",
    "#y = 4*np.square(X[:, 0]) + 1.4*X[:, 1] + 3.5*np.sqrt(np.abs(X[:, 2])) + 0.5*X[:, 3] - 2.7*(X[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "X = np.random.rand(15000, 5)\n",
    "y = sigmoid(0.4*np.square(X[:, 0]) + 0.7*X[:, 1] + 0.5*np.sqrt(np.abs(X[:, 2])) + 0.25*X[:, 3] - 0.17*(X[:, 4]))\n",
    "y = (y>0.7).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Class classification\n",
    "\n",
    "# from tensorflow.keras.datasets import mnist\n",
    "# (X_train, y_train), _ = mnist.load_data()\n",
    "\n",
    "# X = X_train.reshape(X_train.shape[0], -1)\n",
    "# X = X[:15000]\n",
    "# X = X/255.0\n",
    "\n",
    "# y = y_train[:15000]\n",
    "# z = np.zeros((y.shape[0], 10))\n",
    "# for i in range(len(y)):\n",
    "#     z[i, y[i]] = 1\n",
    "# y = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN([5, 10, 7, 1], ['none', 'relu', 'relu', 'none'])\n",
    "# model = NN([5, 10, 7, 1], ['none', 'relu', 'relu', 'sigmoid'])\n",
    "# model = NN([784, 1200, 550, 10], ['none', 'relu', 'relu', 'softmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile_network('mse')\n",
    "# model.compile_network('binary_crossentropy', ['accuracy'])\n",
    "# model.compile_network('categorical_crossentropy', ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 ==> mse: 0.41116456737737656 val_mse: 0.4226586282304884 \n",
      "Epoch 2 ==> mse: 0.3352310917097101 val_mse: 0.3773214144481722 \n",
      "Epoch 3 ==> mse: 0.3021346355300285 val_mse: 0.3182416456637003 \n",
      "Epoch 4 ==> mse: 0.2712493985740437 val_mse: 0.3040422957095154 \n",
      "Epoch 5 ==> mse: 0.2470639251090455 val_mse: 0.2691861422605654 \n",
      "Epoch 6 ==> mse: 0.23107720920681438 val_mse: 0.2583362265859489 \n",
      "Epoch 7 ==> mse: 0.21904000336761562 val_mse: 0.24273881630802097 \n",
      "Epoch 8 ==> mse: 0.21011168880111109 val_mse: 0.2339996362574395 \n",
      "Epoch 9 ==> mse: 0.20265519519818145 val_mse: 0.22568030504375228 \n",
      "Epoch 10 ==> mse: 0.19671856124149062 val_mse: 0.21744626224100475 \n",
      "Epoch 11 ==> mse: 0.19170766254039662 val_mse: 0.21175598973652981 \n",
      "Epoch 12 ==> mse: 0.18758316833537506 val_mse: 0.20557252026883038 \n",
      "Epoch 13 ==> mse: 0.18398935735657315 val_mse: 0.2017323312108726 \n",
      "Epoch 14 ==> mse: 0.18079435072303077 val_mse: 0.1969771481590587 \n",
      "Epoch 15 ==> mse: 0.17831450050941214 val_mse: 0.1939604371332297 \n",
      "Epoch 16 ==> mse: 0.1761193911327111 val_mse: 0.19024160872705184 \n",
      "Epoch 17 ==> mse: 0.17407221967989553 val_mse: 0.18813473692820337 \n",
      "Epoch 18 ==> mse: 0.17239246733092276 val_mse: 0.18546288675626912 \n",
      "Epoch 19 ==> mse: 0.1703654613732814 val_mse: 0.18282765565853004 \n",
      "Epoch 20 ==> mse: 0.1685879581317514 val_mse: 0.18027239403102996 \n",
      "Epoch 21 ==> mse: 0.16688251380057384 val_mse: 0.17796760643955967 \n",
      "Epoch 22 ==> mse: 0.165415473566382 val_mse: 0.17597742186641782 \n",
      "Epoch 23 ==> mse: 0.1640339934768657 val_mse: 0.1742018136559758 \n",
      "Epoch 24 ==> mse: 0.16265680706045285 val_mse: 0.1721994574680406 \n",
      "Epoch 25 ==> mse: 0.16142270724893762 val_mse: 0.17014919632701708 \n",
      "Epoch 26 ==> mse: 0.1601754348700098 val_mse: 0.16836790775563412 \n",
      "Epoch 27 ==> mse: 0.1590059338861054 val_mse: 0.16672302455243876 \n",
      "Epoch 28 ==> mse: 0.1579897807590821 val_mse: 0.16488232813570028 \n",
      "Epoch 29 ==> mse: 0.1569683280808151 val_mse: 0.164143405917877 \n",
      "Epoch 30 ==> mse: 0.15567029911816668 val_mse: 0.1621430486640983 \n",
      "Epoch 31 ==> mse: 0.15444089557382862 val_mse: 0.16034515123230977 \n",
      "Epoch 32 ==> mse: 0.1531462106064617 val_mse: 0.15844629309535324 \n",
      "Epoch 33 ==> mse: 0.15191381563667586 val_mse: 0.15653841272797156 \n",
      "Epoch 34 ==> mse: 0.15083645468377443 val_mse: 0.15480691116507353 \n",
      "Epoch 35 ==> mse: 0.14986461184526775 val_mse: 0.1532440076082582 \n",
      "Epoch 36 ==> mse: 0.1489457140475529 val_mse: 0.15179478027226442 \n",
      "Epoch 37 ==> mse: 0.14794994784596974 val_mse: 0.1504287171946762 \n",
      "Epoch 38 ==> mse: 0.14658377490768776 val_mse: 0.14895006588711004 \n",
      "Epoch 39 ==> mse: 0.1447401320359684 val_mse: 0.14730461804275755 \n",
      "Epoch 40 ==> mse: 0.14322265914826338 val_mse: 0.1460080962528637 \n",
      "Epoch 41 ==> mse: 0.14204852822451128 val_mse: 0.14466869689822898 \n",
      "Epoch 42 ==> mse: 0.14086373666443935 val_mse: 0.14238172426205412 \n",
      "Epoch 43 ==> mse: 0.13998342425832125 val_mse: 0.14147463598365592 \n",
      "Epoch 44 ==> mse: 0.1387026128680493 val_mse: 0.13971621872020307 \n",
      "Epoch 45 ==> mse: 0.13816560771600164 val_mse: 0.1387020213494352 \n",
      "Epoch 46 ==> mse: 0.13687643600733607 val_mse: 0.13728328340067703 \n",
      "Epoch 47 ==> mse: 0.13595013065663908 val_mse: 0.13588288862448153 \n",
      "Epoch 48 ==> mse: 0.13521842747797824 val_mse: 0.13480124372036612 \n",
      "Epoch 49 ==> mse: 0.13429350794485442 val_mse: 0.13374932776956994 \n",
      "Epoch 50 ==> mse: 0.13328968958370216 val_mse: 0.1326114941941834 \n",
      "Epoch 51 ==> mse: 0.13219565098311167 val_mse: 0.13151239926952682 \n",
      "Epoch 52 ==> mse: 0.13085193136746978 val_mse: 0.1301254243130151 \n",
      "Epoch 53 ==> mse: 0.12961010100041406 val_mse: 0.12878615412923494 \n",
      "Epoch 54 ==> mse: 0.1284848186683081 val_mse: 0.12766925231127682 \n",
      "Epoch 55 ==> mse: 0.12743574770484872 val_mse: 0.12643815310842135 \n",
      "Epoch 56 ==> mse: 0.1264036943576938 val_mse: 0.1254555522874289 \n",
      "Epoch 57 ==> mse: 0.12545817396380804 val_mse: 0.12444682180873656 \n",
      "Epoch 58 ==> mse: 0.12454565567381415 val_mse: 0.12371167941369647 \n",
      "Epoch 59 ==> mse: 0.12375637153789898 val_mse: 0.12280408416329429 \n",
      "Epoch 60 ==> mse: 0.12292597943220337 val_mse: 0.12185902057619144 \n",
      "Epoch 61 ==> mse: 0.12221432439989384 val_mse: 0.12147315301675081 \n",
      "Epoch 62 ==> mse: 0.12156349293033347 val_mse: 0.12045130205455398 \n",
      "Epoch 63 ==> mse: 0.12095208078959066 val_mse: 0.11967567454645696 \n",
      "Epoch 64 ==> mse: 0.1203247981693015 val_mse: 0.11897987611652179 \n",
      "Epoch 65 ==> mse: 0.11971049221933061 val_mse: 0.11835317570610164 \n",
      "Epoch 66 ==> mse: 0.11887439220226113 val_mse: 0.11689067065657804 \n",
      "Epoch 67 ==> mse: 0.11828783131605042 val_mse: 0.11616458613130028 \n",
      "Epoch 68 ==> mse: 0.11753538967481644 val_mse: 0.11523921120360521 \n",
      "Epoch 69 ==> mse: 0.11680882221276738 val_mse: 0.11454868138718502 \n",
      "Epoch 70 ==> mse: 0.1158552422564247 val_mse: 0.1134052587401281 \n",
      "Epoch 71 ==> mse: 0.1149777446233195 val_mse: 0.11224090224884091 \n",
      "Epoch 72 ==> mse: 0.1141910769470239 val_mse: 0.1112293116096626 \n",
      "Epoch 73 ==> mse: 0.11344101148449055 val_mse: 0.11079592161386947 \n",
      "Epoch 74 ==> mse: 0.11270787900806312 val_mse: 0.10980019846550661 \n",
      "Epoch 75 ==> mse: 0.11206427863982266 val_mse: 0.10915461870271279 \n",
      "Epoch 76 ==> mse: 0.11136006769568108 val_mse: 0.10850246934661922 \n",
      "Epoch 77 ==> mse: 0.11068706183054047 val_mse: 0.1077254337165724 \n",
      "Epoch 78 ==> mse: 0.10998971242595015 val_mse: 0.1071363887406553 \n",
      "Epoch 79 ==> mse: 0.10932905310831931 val_mse: 0.10645568104337172 \n",
      "Epoch 80 ==> mse: 0.10892753423909783 val_mse: 0.10606121057496791 \n",
      "Epoch 81 ==> mse: 0.10859300400215471 val_mse: 0.1054934788517909 \n",
      "Epoch 82 ==> mse: 0.10790230679611142 val_mse: 0.10551746627862835 \n",
      "Epoch 83 ==> mse: 0.10769994808427487 val_mse: 0.1044504271387222 \n",
      "Epoch 84 ==> mse: 0.10690066771712724 val_mse: 0.10477573092147886 \n",
      "Epoch 85 ==> mse: 0.10636306770022648 val_mse: 0.10320724240631937 \n",
      "Epoch 86 ==> mse: 0.10579406995144765 val_mse: 0.10361572249577984 \n",
      "Epoch 87 ==> mse: 0.1053068777001841 val_mse: 0.1021690488119662 \n",
      "Epoch 88 ==> mse: 0.10461306021366971 val_mse: 0.10258967274361391 \n",
      "Epoch 89 ==> mse: 0.10415853524508707 val_mse: 0.10123002629234643 \n",
      "Epoch 90 ==> mse: 0.10366532834393798 val_mse: 0.10162434770187691 \n",
      "Epoch 91 ==> mse: 0.10328453275844966 val_mse: 0.10052494293838518 \n",
      "Epoch 92 ==> mse: 0.10271543581928645 val_mse: 0.10086257922656096 \n",
      "Epoch 93 ==> mse: 0.10223502792626415 val_mse: 0.09961263339040677 \n",
      "Epoch 94 ==> mse: 0.10195879635503635 val_mse: 0.09977423656539813 \n",
      "Epoch 95 ==> mse: 0.10169113053686946 val_mse: 0.098943576796495 \n",
      "Epoch 96 ==> mse: 0.10130334601443376 val_mse: 0.0993367905202751 \n",
      "Epoch 97 ==> mse: 0.10122661274129413 val_mse: 0.09818674799597311 \n",
      "Epoch 98 ==> mse: 0.10082685223685209 val_mse: 0.09917089133991173 \n",
      "Epoch 99 ==> mse: 0.10083702959072563 val_mse: 0.09748369240904307 \n",
      "Epoch 100 ==> mse: 0.09994462724751027 val_mse: 0.09825635393182293 "
     ]
    }
   ],
   "source": [
    "model.fit(X[:10000], y[:10000], epochs=100, batch_size=100, learning_rate=0.1, validation_data=[X[10000:], y[10000:]])\n",
    "# model.fit(X[:10000], y[:10000], epochs=100, batch_size=100, learning_rate=0.1, validation_data=[X[10000:], y[10000:]])\n",
    "# model.fit(X[:10000], y[:10000], epochs=100, batch_size=100, learning_rate=0.001, validation_data=[X[10000:], y[10000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.7474967 ],\n",
       "       [ 0.74395809],\n",
       "       [ 0.37299054],\n",
       "       ...,\n",
       "       [-0.18000952],\n",
       "       [-0.13340807],\n",
       "       [ 0.56683151]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean square error: 0.3134586957348973\n",
      "R2 score: 0.6069441460273768\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X[10000:], y[10000:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
